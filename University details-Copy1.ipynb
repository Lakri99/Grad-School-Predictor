{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this creates a csv file 'universityList.csv' containing name, key and link for all university.\n",
    "# Note to self\n",
    "# ------- Do not Execute this cell -----------\n",
    "\n",
    "getUniversityList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import urllib.request\n",
    "from requests.exceptions import HTTPError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_url_2_soup(url):\n",
    "    \"\"\"\n",
    "    open webpage and convert to beautiful soup object\n",
    "    returns: page_soup(soup object)\n",
    "    params: url(str) - url of the page\n",
    "    \n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'}\n",
    "    page = requests.get(url,headers = headers,verify = False, allow_redirects = False)\n",
    "    if(page.status_code == 200):\n",
    "        page_soup = soup(page.content,'lxml')\n",
    "        return page_soup\n",
    "    else: \n",
    "        raise HTTPError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUniversityLookup(url):\n",
    "    \"\"\"\n",
    "    get the list of university and it's basic info from ONE page\n",
    "    returns : univ_df - dataframe containing name,key and url of universities one page at a time\n",
    "    params: url - url of the page\n",
    "    \n",
    "    \"\"\"\n",
    "    univ_name = []\n",
    "    univ_link = []\n",
    "    univ_soup = open_url_2_soup(url)\n",
    "    univ_detail_tag = univ_soup.findAll(\"div\",{\"class\":\"col-sm-9 col-xs-12\"})\n",
    "    for tag in univ_detail_tag:\n",
    "        a = tag.find(\"a\",href=True)\n",
    "        univ_name.append(a.text)\n",
    "        univ_link.append(\"https://yocket.in\"+a['href'])\n",
    "    #data cleaning \n",
    "    univ_df = pd.DataFrame({\"name\":np.asarray(univ_name),\"href\": np.asarray(univ_link)})\n",
    "    print(univ_df['name'].str.split('(',n = 1, expand = True))\n",
    "    univ_df[['name','keys']] = univ_df['name'].str.split('(',n = 1, expand = True)\n",
    "    univ_df['keys'] = univ_df['keys'].apply(lambda x: x[:-1])\n",
    "    return univ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUniversityList():\n",
    "    \"\"\"\n",
    "    creates a csv file of all university detail\n",
    "    returns: UniversityList_df - dataframe containg the basic details of top 500 engineering schools\n",
    "    params: None\n",
    "    \n",
    "    \"\"\"\n",
    "    UniversityList_df = pd.DataFrame()\n",
    "    count = 1\n",
    "    while(count <= 16):\n",
    "        url = \"https://yocket.in/universities?page=\" + str(count)\n",
    "        temp_df = getUniversityLookup(url)\n",
    "#         print(temp_df)\n",
    "        UniversityList_df = UniversityList_df.append(temp_df,ignore_index=True)\n",
    "#         print(UniversityList_df)\n",
    "        time.sleep(5) \n",
    "        count += 1\n",
    "    UniversityList_df.to_csv('universityList.csv', encoding='utf-8')\n",
    "    return UniversityList_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getUniversityDetail(url):\n",
    "    \"\"\"\n",
    "    get the details of a single university\n",
    "    params: url(string) - url of the site\n",
    "    returns : univ_detail(dict) - dictionary containing details of a single university\n",
    "    \n",
    "    \"\"\"\n",
    "    univ_detail = {}\n",
    "    page_soup = open_url_2_soup(url)\n",
    "    univ_tag = page_soup.findAll(\"div\",{\"class\":\"col-sm-3 col-xs-6\"})\n",
    "    univ_detail['name'] = page_soup.findAll(\"h1\")[0].text\n",
    "    tag_no = 1;\n",
    "    for tag in univ_tag:\n",
    "        if(tag_no in [1,2,3,4,6,8,9]):\n",
    "            name,data = extractUnivDetails(tag,tag_no)\n",
    "            univ_detail[name] = data\n",
    "        tag_no+= 1\n",
    "    try:\n",
    "        univ_detail['admitList'] = univ_detail['student_link']+\"2\"\n",
    "        univ_detail['rejectList'] = univ_detail['student_link']+\"3\"\n",
    "        univ_detail['appliedList'] = univ_detail['student_link']+\"1\"\n",
    "        del univ_detail['student_link']\n",
    "    except:\n",
    "        univ_detail['admitList'] = None\n",
    "        univ_detail['rejectList'] = None\n",
    "        univ_detail['appliedList'] = None\n",
    "        del univ_detail['student_link']\n",
    "    return(univ_detail)\n",
    "# getUniversityDetail(\"https://yocket.in/university-reviews/university-of-kentucky-2619/engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractUnivDetails(tag,tag_no):\n",
    "    \"\"\"\n",
    "    returns : univ_dict(dictionary) - dictionary containing university(single) detail\n",
    "    params: tag(soup element) - the div containing the element\n",
    "            tag_no(int) - number of div\n",
    "    \"\"\"\n",
    "    univ_dict = {}\n",
    "    if(tag_no in [1,2,8,9]):\n",
    "        return(tag.h3.small.text,(tag.br.previous_sibling)[1:])\n",
    "    elif(tag_no ==3):\n",
    "        return (tag.h3.findAll(\"small\")[1].text,tag.h3.find(\"small\").previous_sibling)\n",
    "    elif(tag_no ==4):\n",
    "        return (tag.h3.findAll(\"small\")[1].text,(tag.h3.find(\"small\").previous_sibling)[2:])\n",
    "    elif(tag_no == 6):\n",
    "        try:\n",
    "            a = tag.findAll(\"a\",href=True)[0]\n",
    "        except:\n",
    "            return(\"student_link\",None)\n",
    "        else:\n",
    "            return(\"student_link\",(\"https://yocket.in\"+a['href'])[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def univDetail_to_csv():\n",
    "    \"\"\"\n",
    "    generate csv file containing university details\n",
    "    params : None\n",
    "    returns : None\n",
    "    \"\"\"\n",
    "    univ_list = pd.read_csv('universityList.csv')\n",
    "    universityDetail = pd.DataFrame()\n",
    "    counter = 51\n",
    "    while(counter <= 100):\n",
    "        link = univ_list.iloc[counter]['href']\n",
    "        dict_temp = getUniversityDetail(link)\n",
    "        universityDetail = universityDetail.append(dict_temp,ignore_index = True)\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        time.sleep(10)\n",
    "#     print(universityDetail)\n",
    "    universityDetail.to_csv('universityDetailList.csv', encoding='utf-8',header = False,mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the cab file of university list with all it's parameter\n",
    "#  Note to self\n",
    "# -----------DO NOT EXECUTE THIS CELL --------------\n",
    "univDetail_to_csv()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStdList(page_soup,admit_status):\n",
    "    name_tag = page_soup.findAll(\"input\" , {\"id\" : \"users-view-search-universities\"})[0]\n",
    "    name = name_tag['value']\n",
    "    grading = page_soup.findAll(\"div\",{\"class\" : \"col-sm-6\"})[1:]\n",
    "#     print(grading[0])\n",
    "    student_df = pd.DataFrame()\n",
    "    for tag in grading:\n",
    "#         print(tag)\n",
    "        std_data = {}\n",
    "        std_data['college'] = name\n",
    "        std_data['admit/reject'] = admit_status\n",
    "        a = tag.findAll(\"a\",href = True)[0]\n",
    "#         print(a)\n",
    "        std_data['Name'] = a.text #Tag for name of student\n",
    "#         std_data['studentLink'] - a['href']\n",
    "        std_data['CourseName'] = tag.find(\"small\").text #Tag for course\n",
    "        student_data_tag = tag.findAll(\"div\",{\"class\":\"col-sm-3 col-xs-6\"})\n",
    "        for data_tag in student_data_tag:\n",
    "            heading = data_tag.find(\"strong\").text\n",
    "            data = data_tag.findNext(\"br\").next_sibling\n",
    "            if (heading == \"GRE\"):\n",
    "                std_data['GRE'] = data.string\n",
    "\n",
    "            elif(heading == \"TOEFL\" or heading == \"IELTS\" or heading == \"ENG TEST\"):\n",
    "                std_data['ENG_TEST'] = data.string\n",
    "\n",
    "            elif(\"UNDERGRAD\"):\n",
    "                std_data['CGPA']= data.string\n",
    "\n",
    "            elif(\"WORK EX\"):\n",
    "                std_data['WorkExp'] = data.string\n",
    "#         print(std_data)\n",
    "        student_df = student_df.append(std_data, ignore_index=True)\n",
    "#     print(student_df)\n",
    "    return student_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStudentDetailList(url):\n",
    "    page_counter = 1\n",
    "    student_detail = pd.DataFrame()\n",
    "    url = url + \"?page=4\"\n",
    "    while(True):\n",
    "        \n",
    "        print(url)\n",
    "        try:\n",
    "            page_soup = open_url_2_soup(url)\n",
    "        except HTTPError:\n",
    "            print(\"exception raised\")\n",
    "            page_counter = 1\n",
    "            break\n",
    "        else:\n",
    "            print(\"in else\")\n",
    "            std_detail_page = getStdList(page_soup,'admit')\n",
    "            student_detail = pd.concat([student_detail,std_detail_page])\n",
    "            page_counter += 1\n",
    "            url = url[:-1] + str(page_counter)\n",
    "        \n",
    "#     print(student_detail.head(4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://yocket.in/applications-admits-rejects/50461-eindhoven-university-of-technology/2?page=4\n",
      "exception raised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbl/anaconda3/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "u = \"https://yocket.in/applications-admits-rejects/50461-eindhoven-university-of-technology/2\"\n",
    "# page_soup = open_url_2_soup(u)\n",
    "# grading = page_soup.findAll(\"div\",{\"class\" : \"col-sm-6\"})[]\n",
    "# print(grading)\n",
    "getStudentDetailList(u) \n",
    "#name\n",
    "# print(data['value']) #name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbl/anaconda3/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web site does not exist\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'}\n",
    "url = \"https://yocket.in/applications-admits-rejects/50461-eindhoven-university-of-technology/2?page=2\"\n",
    "request = requests.get(url , headers = headers, verify = False, allow_redirects=False)\n",
    "if request.status_code == 200:\n",
    "    print('Web site exists')\n",
    "else:\n",
    "    print('Web site does not exist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
